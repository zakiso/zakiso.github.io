<!DOCTYPE html>
<html class="has-navbar-fixed-top">
<head><meta name="generator" content="Hexo 3.9.0">
    <meta charset="utf-8">
<title>PillBlog</title>
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">

<link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/outdated-browser/1.1.5/outdatedbrowser.min.css">








    <meta property="og:type" content="website">
<meta property="og:title" content="PillBlog">
<meta property="og:url" content="https://apppills.com/page/24/index.html">
<meta property="og:site_name" content="PillBlog">
<meta property="og:locale" content="zh-CN">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="PillBlog">





<link rel="icon" href="/favicon.png">


<link rel="stylesheet" href="//fonts.googleapis.com/css?family=Ovo|Source+Code+Pro">
<link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/bulma/0.6.2/css/bulma.min.css">


<link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/lightgallery/1.6.8/css/lightgallery.min.css">
<link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/justifiedGallery/3.6.5/css/justifiedGallery.min.css">


<link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/atom-one-light.min.css">

<link rel="stylesheet" href="/css/style.css">

<script defer src="//use.fontawesome.com/releases/v5.0.8/js/all.js"></script>


    
    
    
    
    
    
    
    
    
    

    


</head>
<body>
    
<nav class="navbar is-transparent is-fixed-top navbar-main" role="navigation" aria-label="main navigation">
    <div class="container">
        <div class="navbar-brand">
            <a class="navbar-item navbar-logo" href="/">
                
                <img src="/images/logo.png" alt height="28">
                
            </a>
            <div class="navbar-burger">
                <span></span>
                <span></span>
                <span></span>
            </div>
        </div>
        
        <div class="navbar-menu navbar-start">
            
            <a class="navbar-item " href="/archives">归档</a>
            
            <a class="navbar-item " href="/about">关于</a>
            
            <a class="navbar-item " href="https://everything.apppills.com">Everything</a>
            
        </div>
        
        <div class="navbar-menu navbar-end">
            
            <a class="navbar-item search" title="Search" href="javascript:;">
                <i class="fas fa-search"></i>
            </a>
            
            
            
            <a class="navbar-item" title="GitHub" href="https://github.com/ppoffice/hexo-theme-minos">
                
                <i class="fab fa-github"></i>
                
            </a>
               
            
        </div>
    </div>
</nav>

    <section class="section">
    <div class="container">
    
        <article class="article content gallery" itemscope itemprop="blogPost">
    <h1 class="article-title is-size-3 is-size-4-mobile" itemprop="name">
        
            <a href="/2014/09/03/android/os/binder/2014-09-03-Binder-ServiceManager-Daemon/" itemprop="url">Android Binder机制(三) ServiceManager守护进程</a>
        
    </h1>
    <div class="article-meta columns is-variable is-1 is-multiline is-mobile is-size-7-mobile">
        <span class="column is-narrow">
            <time datetime="2014-09-03T01:01:00.000Z" itemprop="datePublished">9月 3 2014</time>
        </span>
        
        <span class="column is-narrow article-category">
            <i class="far fa-folder"></i>
            <a class="article-category-link" href="/categories/android/">android</a>
        </span>
        
        
        <span class="column is-narrow">
            
            
            1 小时 read (About 8281 words)
        </span>
        
    </div>
    <div class="article-entry is-size-6-mobile" itemprop="articleBody">
    
        <blockquote>
<p>ServiceManager是用户空间的一个守护进程，它一直运行在后台。它的职责是管理Binder机制中的各个Server。当Server启动时，Server会将”Server对象的名字”连同”Server对象的信息”一起注册到ServiceManager中；而当Client需要获取Server接入点时，则通过”Server的名字”来从ServiceManager中找到对应的Server。<br>本文的主要内容就是对ServiceManager进行介绍，通过它的启动流程来分析它是如何成为Server管理者的。</p>
</blockquote>
<blockquote>
<p>注意：本文是基于Android 4.4.2版本进行介绍的！</p>
</blockquote>
<blockquote>
<p><strong>目录</strong><br><strong>1</strong>. <a href="#anchor_1st">ServiceManager流程图</a><br><strong>2</strong>. <a href="#anchor_2nd">ServiceManager流程详解</a><br><strong>2.1</strong>. <a href="#anchor1">main()</a><br><strong>2.2</strong>. <a href="#anchor2">binder_open()</a><br><strong>2.3</strong>. <a href="#anchor3">open(“/dev/binder”)</a><br><strong>2.4</strong>. <a href="#anchor4">mmap()</a><br><strong>2.5</strong>. <a href="#anchor5">binder_become_context_manager()</a><br><strong>2.6</strong>. <a href="#anchor6">ioctl(, BINDER_SET_CONTEXT_MGR,)</a><br><strong>2.7</strong>. <a href="#anchor7">binder_loop()</a><br><strong>2.8</strong>. <a href="#anchor8">for(;;)</a><br><strong>3</strong>. <a href="#anchor_3rd">ServiceManager流程总结</a>  </p>
</blockquote>
<p><a name="anchor_1st"></a></p>
<h1 id="ServiceManager流程图"><a href="#ServiceManager流程图" class="headerlink" title="ServiceManager流程图"></a>ServiceManager流程图</h1><p><a href="https://raw.githubusercontent.com/wangkuiwu/android_applets/master/os/pic/binder/ServiceManager.jpg" target="_blank" rel="noopener"><img src="https://raw.githubusercontent.com/wangkuiwu/android_applets/master/os/pic/binder/ServiceManager.jpg" alt></a></p>
<p>上面是ServiceManager的时序图。它启动之后，会先打开”/dev/binder”文件(“/dev/binder”是Binder驱动注册的设备节点)。打开文件之后，再告诉Binder驱动，它是Binder的上下文管理者。之后，就进入到了消息循环中。进入消息循环之后，会不断的从Binder的待处理事务队列中读取事务(Binder请求或反馈)，读出事务之后就进行解析，然后交给相应的进程进行处理。若没有事务，则进入等待状态，等待被唤醒。</p>
<p><a name="anchor_2nd"></a></p>
<h1 id="ServiceManager流程详解"><a href="#ServiceManager流程详解" class="headerlink" title="ServiceManager流程详解"></a>ServiceManager流程详解</h1><p><a name="anchor1"></a></p>
<h2 id="1-main"><a href="#1-main" class="headerlink" title="1. main()"></a>1. main()</h2><p>ServiceManager是一个守护进程。它的main()函数源码如下：</p>
<pre><code>int main(int argc, char **argv)
{
    struct binder_state *bs;
    void *svcmgr = BINDER_SERVICE_MANAGER;

    bs = binder_open(128*1024);

    if (binder_become_context_manager(bs)) {
        ALOGE(&quot;cannot become context manager (%s)\n&quot;, strerror(errno));
        return -1; 
    }   

    svcmgr_handle = svcmgr;
    binder_loop(bs, svcmgr_handler);
    return 0;
}</code></pre><p>说明：该代码在frameworks/native/cmds/servicemanager/service_manager.c中。main()主要进行了三项工作：<br>(01) 通过binder_open()打开”/dev/binder”文件，即打开Binder设备文件。<br>(02) 调用binder_become_context_manager()，通过ioctl()告诉Binder驱动程序自己是Binder上下文管理者。<br>(03) 调用binder_loop()进入消息循环，等待Client的请求。如果没有Client请求，则进入中断等待状态；当有Client请求时，就被唤醒，然后读取并处理Client请求。  </p>
<blockquote>
<p>ServiceManager是如何启动的？<br>这里简要介绍一下ServiceManager的启动方式。当Kernel启动加载完驱动之后，会启动Android的init程序，init程序会解析init.rc，进而启动init.rc中定义的守护进程。而ServiceManager则正是通过注册在init.rc中，而被启动的。 </p>
</blockquote>
<p><a name="anchor2"></a></p>
<h2 id="2-binder-open"><a href="#2-binder-open" class="headerlink" title="2. binder_open()"></a>2. binder_open()</h2><p>下面，对main()的逐个步骤进行详细分析。先看看binder_open()，代码如下：</p>
<pre><code>struct binder_state *binder_open(unsigned mapsize)
{
    struct binder_state *bs;

    bs = malloc(sizeof(*bs));
    ...

    bs-&gt;fd = open(&quot;/dev/binder&quot;, O_RDWR);
    ...

    bs-&gt;mapsize = mapsize;
    bs-&gt;mapped = mmap(NULL, mapsize, PROT_READ, MAP_PRIVATE, bs-&gt;fd, 0); 
    ...

    return bs;
}</code></pre><p>说明： 该代码定义在frameworks/native/cmds/servicemanager/binder.c中。binder_open的作用是打开”/dev/binder”设备文件，然后调用mmap()将设备文件”/dev/binder”映射到进程空间的起始地址。<br>(01) open(“/dev/binder”, O_RDWR)对应会调用驱动的open函数。<br>(02) mmap(NULL, mapsize, PROT_READ, MAP_PRIVATE, bs-&gt;fd, 0)对应会调用驱动的mmap函数。第一个参数是映射内存的起始地址，NULL代表让系统自动选定地址；mapsize大小是128*1024B，即128K；PROT_READ表示映射区域是可读的；MAP_PRIVATE表示建立一个写入时拷贝的私有映射，即，当进程中对该内存区域进行写入时，是写入到映射的拷贝中；bs-&gt;fd是”/dev/binder”句柄；而0表示偏移。<br>(03) binder_state结构体是来保存/dev/binder设备信息的。其中，fd是用来保存文件句柄，mmaped是映射内存的起始地址，mapsize映射内存大小。</p>
<p><a name="anchor3"></a></p>
<h2 id="3-open-“-dev-binder”"><a href="#3-open-“-dev-binder”" class="headerlink" title="3. open(“/dev/binder”)"></a>3. open(“/dev/binder”)</h2><h3 id="3-1-Binder驱动注册信息"><a href="#3-1-Binder驱动注册信息" class="headerlink" title="3.1 Binder驱动注册信息"></a>3.1 Binder驱动注册信息</h3><p>下面看看open(“/dev/binder”, O_RDWR)到底做了些什么。先看看下面的代码：</p>
<pre><code>static const struct file_operations binder_fops = {
  .owner = THIS_MODULE,
  .poll = binder_poll,
  .unlocked_ioctl = binder_ioctl,
  .mmap = binder_mmap,
  .open = binder_open,
  .flush = binder_flush,
  .release = binder_release,
};

static struct miscdevice binder_miscdev = {
  .minor = MISC_DYNAMIC_MINOR,
  .name = &quot;binder&quot;,
  .fops = &amp;binder_fops
};

static int __init binder_init(void)
{
    ...
    ret = misc_register(&amp;binder_miscdev);
    ...
}

device_initcall(binder_init);</code></pre><p>说明：上面是Kernel中Binder驱动代码，定义在drivers/staging/android/binder.c中。<br>(01) device_initcall(binder_init)的作用是将binder_init()函数注册到Kernel的初始化函数列表中。当Kernel启动后，会按照一定的次序调用初始化函数列表，也就会执行binder_init()函数；执行binder_init()时便会加载Binder驱动。<br>(02) binder_init()函数中会通过misc_register(&amp;binder_miscdev)将Binder驱动注册到文件节点”/dev/binder”上。在Linux中，一切都是文件！将Binder驱动注册到文件节点上之后，就可以通过操作文件节点进而对Binder驱动进行操作。而该文件节点”/dev/binder”的设备信息是binder_miscdev这个结构体对象。<br>(03) binder_miscdev变量是struct miscdevice类型。minor是次设备号，这个我们不需要关心；name是Binder驱动对应在/dev虚拟文件系统下的设备节点名称，也就是/dev/binder中的”binder”；fops是该设备节点的文件操作对象，它是我们需要重点关注的！fops指向binder_fops变量。<br>(04) binder_fops变量是struct file_operations类型。owner是标明了该文件操作变量的拥有者，就是该驱动；poll则指定了poll函数指针，当我们对/dev/binder文件节点执行poll()操作时，实际上就是调用的binder_poll()函数；同理，mmap()对应binder_mmap()，open()对应binder_open()，ioctl()对应binder_ioctl()…</p>
<h3 id="3-2-Binder驱动中的binder-open-函数源码"><a href="#3-2-Binder驱动中的binder-open-函数源码" class="headerlink" title="3.2 Binder驱动中的binder_open()函数源码"></a>3.2 Binder驱动中的binder_open()函数源码</h3><p>经过上面的介绍，我们可以知道open(“/dev/binder”, O_RDWR)实际上是调用Binder驱动中的binder_open()函数。  </p>
<pre><code>static HLIST_HEAD(binder_procs);
...

static int binder_open(struct inode *nodp, struct file *filp)
{
    struct binder_proc *proc;

    binder_debug(BINDER_DEBUG_OPEN_CLOSE, &quot;binder_open: %d:%d\n&quot;,
           current-&gt;group_leader-&gt;pid, current-&gt;pid);

    // 为proc分配内存
    proc = kzalloc(sizeof(*proc), GFP_KERNEL);
    if (proc == NULL)
      return -ENOMEM;
    get_task_struct(current);
    // 将proc-&gt;tsk指向当前线程
    proc-&gt;tsk = current;
    // 初始化proc的待处理事务列表
    INIT_LIST_HEAD(&amp;proc-&gt;todo);
    // 初始化proc的等待队列
    init_waitqueue_head(&amp;proc-&gt;wait);
    // 设置proc的进程优先级为当前线程的优先级
    proc-&gt;default_priority = task_nice(current);

    binder_lock(__func__);

    binder_stats_created(BINDER_STAT_PROC);
    // 将该进程上下文信息proc保存到&quot;全局哈希表binder_procs&quot;中
    hlist_add_head(&amp;proc-&gt;proc_node, &amp;binder_procs);
    // 设置进程id
    proc-&gt;pid = current-&gt;group_leader-&gt;pid;
    INIT_LIST_HEAD(&amp;proc-&gt;delivered_death);
    // 将proc添加到私有数据中。
    // 这样，mmap(),ioctl()等函数都可以通过私有数据获取到proc，即该进程的上下文信息
    filp-&gt;private_data = proc;

    binder_unlock(__func__);

    if (binder_debugfs_dir_entry_proc) {
      char strbuf[11];
      snprintf(strbuf, sizeof(strbuf), &quot;%u&quot;, proc-&gt;pid);
      proc-&gt;debugfs_entry = debugfs_create_file(strbuf, S_IRUGO,
          binder_debugfs_dir_entry_proc, proc, &amp;binder_proc_fops);
    }

    return 0;
}</code></pre><p>说明：binder_proc是记录进程上下文信息的结构体，它的详细介绍请参考<a href="/2014/09/02/Binder-Datastruct/">Android Binder机制(二) Binder中的数据结构</a>。该函数的作用如下。<br>(01) 创建并初始化binder_proc结构体变量proc。binder_proc是描述Binder进程的上下文信息结构体。这里，就是将ServiceManager这个进程的信息都存储到proc中。<br>(02) 将proc添加到全局哈希表binder_procs中。binder_procs不是我们关注的重点，也就不多说了。<br>(03) 将proc设为filp的私有成员。这样，在mmap()，ioctl()等函数中，我们都可以根据filp的私有成员来获取proc信息。  </p>
<p><a name="anchor4"></a></p>
<h2 id="4-mmap"><a href="#4-mmap" class="headerlink" title="4. mmap()"></a>4. mmap()</h2><p>分析完了open()，接下来看看mmap()。mmap()对应会调用Binder驱动的binder_mmap()函数。</p>
<h3 id="4-1-Binder驱动中的binder-mmap-源码"><a href="#4-1-Binder驱动中的binder-mmap-源码" class="headerlink" title="4.1 Binder驱动中的binder_mmap()源码"></a>4.1 Binder驱动中的binder_mmap()源码</h3><pre><code>static int binder_mmap(struct file *filp, struct vm_area_struct *vma)
{
  int ret;
  struct vm_struct *area;
  struct binder_proc *proc = filp-&gt;private_data;
  const char *failure_string;
  struct binder_buffer *buffer;

  // 有效性检查：映射的内存不能大于4M
  if ((vma-&gt;vm_end - vma-&gt;vm_start) &gt; SZ_4M)
      vma-&gt;vm_end = vma-&gt;vm_start + SZ_4M;

  ...

  vma-&gt;vm_flags = (vma-&gt;vm_flags | VM_DONTCOPY) &amp; ~VM_MAYWRITE;

  mutex_lock(&amp;binder_mmap_lock);

  // 获取空闲的内核空间地址
  area = get_vm_area(vma-&gt;vm_end - vma-&gt;vm_start, VM_IOREMAP);
  ...

  // 将内核空间地址赋值给proc-&gt;buffer，即保存到进程上下文中
  proc-&gt;buffer = area-&gt;addr;
  // 计算 &quot;内核空间地址&quot; 和 &quot;进程虚拟地址&quot; 的偏移
  proc-&gt;user_buffer_offset = vma-&gt;vm_start - (uintptr_t)proc-&gt;buffer;
  mutex_unlock(&amp;binder_mmap_lock);

  // 为proc-&gt;pages分配内存
  proc-&gt;pages = kzalloc(sizeof(proc-&gt;pages[0]) * ((vma-&gt;vm_end - vma-&gt;vm_start) / PAGE_SIZE), GFP_KERNEL);
  ...

  // 内核空间的内存大小 = 进程虚拟地址区域(用户空间)的内存大小
  proc-&gt;buffer_size = vma-&gt;vm_end - vma-&gt;vm_start;

  vma-&gt;vm_ops = &amp;binder_vm_ops;
  // 将 proc(进程上下文信息) 赋值给vma私有数据
  vma-&gt;vm_private_data = proc;

  // 通过调用binder_update_page_range()来分配物理页面。
  // 即，将物理内存映射到内核空间 以及 用户空间
  if (binder_update_page_range(proc, 1, proc-&gt;buffer, proc-&gt;buffer + PAGE_SIZE, vma)) {
      goto err_alloc_small_buf_failed;
  }
  buffer = proc-&gt;buffer;
  INIT_LIST_HEAD(&amp;proc-&gt;buffers);
  // 将物理内存添加到proc-&gt;buffers链表中进行管理。
  list_add(&amp;buffer-&gt;entry, &amp;proc-&gt;buffers);
  buffer-&gt;free = 1;
  binder_insert_free_buffer(proc, buffer);
  proc-&gt;free_async_space = proc-&gt;buffer_size / 2;
  barrier();
  proc-&gt;files = get_files_struct(proc-&gt;tsk);
  // 将用户空间地址信息保存到proc中
  proc-&gt;vma = vma;
  proc-&gt;vma_vm_mm = vma-&gt;vm_mm;

  return 0;
  ...
}</code></pre><p>说明：mmap的作用是进行内存映射。当应用调用mmap()映射内存到进程虚拟地址时，该函数会进行两个操作：第一，将指定大小的”物理内存” 映射到 “用户空间”(即，进程的虚拟地址中)。 第二，将该”物理内存” 也映射到 “内核空间(即，内核的虚拟地址中)”。<br>  简单来说，就是”将进程虚拟地址空间和内核虚拟地址空间映射同一个物理页面”。为什么要这么做呢？这就是Binder进程间通信机制的精髓所在了！在讲解之前，先回顾一下进程间通信的基础知识。</p>
<blockquote>
<p>在32位Linux系统的内存地址划分中，0<del>3G为用户空间，3</del>4G为内核空间。应用程序都运行在用户空间，而kernel和驱动都运行在内核空间。应用程序之间若涉及到数据交换(例如，Client进程向Server进程发送请求)，即进程间通信，需要使用管道/消息队列/Socket/共享内存等IPC机制。共享内存控制比较复杂，而Socket常用于网络通信，这里将它们排除；剩下的就是管道/消息队列。下面对管道/消息队列的IPC等通信方式进行介绍。假如现在采用管道/消息队列从Client向Server发送请求，需要先将Client进程的数据拷贝到内核空间，然后再从内核空间拷贝到Server进程中。这其中，总共涉及到了2次内存拷贝！<br>而Binder机制则只需要进行1次内存拷贝即可！</p>
</blockquote>
<p>在Binder通信机制中，mmap()会将Server进程的虚拟地址和内核虚拟地址映射到同一个物理页面。那么当Client进程向Server进程发送请求时，只需要将Client的数据拷贝到内核空间即可！由于Server进程的地址和内核空间映射到同一个物理页面，因此，Client中的数据拷贝到内核空间时，也就相当于拷贝到了Server进程中。因此，Binder通信机制中，数据传输时，只需要1次内存拷贝！</p>
<br>
有了上面的理论基础，再来看mmap()是如何实现的。  
(01) proc = flip->private_data。该flip的私有数据是在binder_open()中设置的，这里通过该私有数据就获取binder_proc变量proc。  
(02) area = get_vm_area(vma->vm_end - vma->vm_start, VM_IOREMAP)。 它的作用是从内核虚拟地址中，获取指定大小的空闲地址，将空闲地址的起始地址赋值给area。 area是vm_struct类型，vm_struct是描述内核虚拟地址信息的结构体。此外，vm_area_struct则是描述进程虚拟地址信息的结构体。  
(03) 接着，给proc->buffer(内核空间地址)，proc->user_buffer_offset(内核空间地址和进程虚拟地址的偏移值)，proc->pages(内核空间所占物理页面的数目)，proc->buffer_size(内核地址空间的大小)赋值。  
(04) 然后，调用binder_update_page_range(proc, 1, proc->buffer, proc->buffer + PAGE_SIZE, vma)。它作用是分配物理内存，下面看看它的实现。  


<h3 id="4-2-Binder驱动中的binder-update-page-range-源码"><a href="#4-2-Binder驱动中的binder-update-page-range-源码" class="headerlink" title="4.2 Binder驱动中的binder_update_page_range()源码"></a>4.2 Binder驱动中的binder_update_page_range()源码</h3><pre><code>static int binder_update_page_range(struct binder_proc *proc, int allocate,
                  void *start, void *end,
                  struct vm_area_struct *vma)
{
  void *page_addr;
  unsigned long user_page_addr;
  struct vm_struct tmp_area;
  struct page **page;

  ...

  // 分配物理页面，
  // 并将&quot;内核空间&quot;和&quot;用户空间(进程的内存区域)&quot;指向同一块物理内存。
  for (page_addr = start; page_addr &lt; end; page_addr += PAGE_SIZE) {
      int ret;
      struct page **page_array_ptr;
      page = &amp;proc-&gt;pages[(page_addr - proc-&gt;buffer) / PAGE_SIZE];

      // 分配物理页面
      *page = alloc_page(GFP_KERNEL | __GFP_ZERO);
      ...

      tmp_area.addr = page_addr;
      tmp_area.size = PAGE_SIZE + PAGE_SIZE /* guard page? */;
      page_array_ptr = page;
      // 将物理页面映射到内核空间中
      ret = map_vm_area(&amp;tmp_area, PAGE_KERNEL, &amp;page_array_ptr);
      ...

      user_page_addr =
          (uintptr_t)page_addr + proc-&gt;user_buffer_offset;
      // 将物理页面映射插入到进程的虚拟内存中
      ret = vm_insert_page(vma, user_page_addr, page[0]);
      ...
  }

  return 0;

  ...
}</code></pre><p>说明： binder_update_page_range()既可分配物理页面，也可以释放物理页面。当参数allocate=1时，会执行分配物理页面的操作；否则，会执行释放物理页面的操作。这里，allocate=1；因此，我们只关心分配物理页面的部分。<br>在for循环中，每分配一个物理页面都会先通过map_vm_area()将该物理内存映射到内核虚拟地址中；然后再将该物理页面插入到进程的虚拟地址空间。</p>
<p><br>至此，binder_open(128<em>1024)算是介绍完了。从”用户空间的ServiceManager进程” 和 “Binder驱动”这两个方面分析它的作用。<br>(01) *</em>ServiceManager进程<strong>：就是打开/dev/binder，同时映射物理内存到进程空间。<br>(02) **Binder驱动</strong>：新建并初始化该进程对应的binder_proc结构体，同时将内核虚拟地址和该进程的虚拟地址映射到同一物理内存中。</p>
<p><a name="anchor5"></a></p>
<h2 id="5-binder-become-context-manager"><a href="#5-binder-become-context-manager" class="headerlink" title="5. binder_become_context_manager()"></a>5. binder_become_context_manager()</h2><p>下面接着分析binder_become_context_manager(bs)。</p>
<pre><code>int binder_become_context_manager(struct binder_state *bs)
{
    return ioctl(bs-&gt;fd, BINDER_SET_CONTEXT_MGR, 0);
}</code></pre><p>说明：根据前面介绍的Binder驱动初始化信息可知，ioctl()就是调用Binder驱动中的binder_ioctl()函数。 </p>
<p><a name="anchor6"></a></p>
<h2 id="6-ioctl-BINDER-SET-CONTEXT-MGR"><a href="#6-ioctl-BINDER-SET-CONTEXT-MGR" class="headerlink" title="6. ioctl(, BINDER_SET_CONTEXT_MGR,)"></a>6. ioctl(, BINDER_SET_CONTEXT_MGR,)</h2><h3 id="6-1-Binder驱动中binder-ioctl-的BINDER-WRITE-READ相关部分的源码"><a href="#6-1-Binder驱动中binder-ioctl-的BINDER-WRITE-READ相关部分的源码" class="headerlink" title="6.1 Binder驱动中binder_ioctl()的BINDER_WRITE_READ相关部分的源码"></a>6.1 Binder驱动中binder_ioctl()的BINDER_WRITE_READ相关部分的源码</h3><pre><code>// 全局binder实体，准确点说是ServiceManager的binder实体
static struct binder_node *binder_context_mgr_node;
// ServiceManager守护进程的uid
static uid_t binder_context_mgr_uid = -1;
static int binder_stop_on_user_error;
...

static long binder_ioctl(struct file *filp, unsigned int cmd, unsigned long arg)
{
  int ret;
  struct binder_proc *proc = filp-&gt;private_data;
  struct binder_thread *thread;
  unsigned int size = _IOC_SIZE(cmd);
  void __user *ubuf = (void __user *)arg;

  // 中断等待函数。
  // 1. 当binder_stop_on_user_error &lt; 2为true时；不会进入等待状态；直接跳过。
  // 2. 当binder_stop_on_user_error &lt; 2为false时，进入等待状态。
  //    当有其他进程通过wake_up_interruptible来唤醒binder_user_error_wait队列，并且binder_stop_on_user_error &lt; 2为true时；
  //    则继续执行；否则，再进入等待状态。
  ret = wait_event_interruptible(binder_user_error_wait, binder_stop_on_user_error &lt; 2);
  ...

  binder_lock(__func__);
  // 在proc进程中查找该线程对应的binder_thread；若查找失败，则新建一个binder_thread，并添加到proc-&gt;threads中。
  thread = binder_get_thread(proc);
  ...

  switch (cmd) {
  ...

  case BINDER_SET_CONTEXT_MGR:
      if (binder_context_mgr_node != NULL) {
          ...
      }
      if (binder_context_mgr_uid != -1) {
          ...
      } else
          // 设置ServiceManager对应的uid
          binder_context_mgr_uid = current-&gt;cred-&gt;euid;

      // 新建binder实体，并将proc进程上下文信息保存到binder实体中；
      // 然后，将该binder实体赋值给全局变量binder_context_mgr_node。
      // 这个全局的binder实体，是ServiceManager对应的binder实体。
      binder_context_mgr_node = binder_new_node(proc, NULL, NULL);
      ...

      // 设置binder实体的引用计数等参数
      binder_context_mgr_node-&gt;local_weak_refs++;
      binder_context_mgr_node-&gt;local_strong_refs++;
      binder_context_mgr_node-&gt;has_strong_ref = 1;
      binder_context_mgr_node-&gt;has_weak_ref = 1;
      break;
  ...

  }
  ret = 0;

err:
  // 去掉thread的BINDER_LOOPER_STATE_NEED_RETURN标记
  if (thread)
      thread-&gt;looper &amp;= ~BINDER_LOOPER_STATE_NEED_RETURN;
  binder_unlock(__func__);
  wait_event_interruptible(binder_user_error_wait, binder_stop_on_user_error &lt; 2);
  ...

  return ret;
}</code></pre><p>说明：binder_ioctl()的内容很多，上面仅仅列出与BINDER_SET_CONTEXT_MGR相关的代码。<br>(01) proc = flip-&gt;private_data。该flip的私有数据是在binder_open()中设置的，这里通过该私有数据就获取binder_proc变量proc。<br>(02) 接着调用wait_event_interruptible(binder_user_error_wait, binder_stop_on_user_error &lt; 2)。由于binder_stop_on_user_error是全局变量，它的初始值是0，因此binder_stop_on_user_error &lt; 2为true，不进入中断等待，而是直接跳过该函数继续运行。<br>(03) binder_get_thread()会在proc中查找当前线程对应的binder_thread结构体；由于之前还未创建该线程的binder_thread结构体，因此查找失败。进而创建一个binder_thread结构体变量，并将其添加到proc-&gt;threads红黑树中，然后返回该变量。<br>(04) cmd的值是我们调用ioctl()传入的参数BINDER_SET_CONTEXT_MGR。在BINDER_SET_CONTEXT_MGR分支中，会设置binder_context_mgr_uid，binder_context_mgr_uid是一个全局变量，它代表ServiceManager对应的uid；接着，通过binder_new_node()新建一个Binder实体(即binder_node结构体对象)，并将该Binder实体赋值给全局变量binder_context_mgr_node，binder_context_mgr_node就是Serveice Manager对应的Binder实体；最后，设置binder实体的引用计数等参数。<br>(05) 清除thread-&gt;looper的BINDER_LOOPER_STATE_NEED_RETURN标记。这个BINDER_LOOPER_STATE_NEED_RETURN标记，是在调用binder_get_thread()中创建binder_thread对象时添加的。  </p>
<p>关于binder_node结构体，在<a href="/2014/09/02/Binder-Datastruct/">Android Binder机制(二) Binder中的数据结构</a>中有消息的介绍。特别需要了解的是，对于每一个Server，Binder驱动都会为其分配一个binder_node对象。对于ServiceManager这个Binder上下文管理者而言，Binder驱动更是会将它的Binder实体保存到全局变量中。</p>
<h3 id="6-2-Binder驱动中的binder-get-thread-源码"><a href="#6-2-Binder驱动中的binder-get-thread-源码" class="headerlink" title="6.2 Binder驱动中的binder_get_thread()源码"></a>6.2 Binder驱动中的binder_get_thread()源码</h3><p>下面看看binder_get_thread()中做了什么。</p>
<pre><code>static struct binder_thread *binder_get_thread(struct binder_proc *proc)
{
  struct binder_thread *thread = NULL;
  struct rb_node *parent = NULL;
  struct rb_node **p = &amp;proc-&gt;threads.rb_node;

  // 在proc-&gt;threads这棵红黑树中，查找是否有线程的pid和current-&gt;pid相同。
  // 即，查找当前线程中是否创建过binder_thread信息
  while (*p) {
      parent = *p;
      thread = rb_entry(parent, struct binder_thread, rb_node);

      if (current-&gt;pid &lt; thread-&gt;pid)
          p = &amp;(*p)-&gt;rb_left;
      else if (current-&gt;pid &gt; thread-&gt;pid)
          p = &amp;(*p)-&gt;rb_right;
      else
          break;
  }
  // 若当前线程中没有创建过binder_thread信息；
  // 则创建binder_thread，并初始化；然后将其添加到binder_proc进程的proc-&gt;threads中
  if (*p == NULL) {
      thread = kzalloc(sizeof(*thread), GFP_KERNEL);
      if (thread == NULL)
          return NULL;
      binder_stats_created(BINDER_STAT_THREAD);
      // 将进程的上下文信息保存到thread中
      thread-&gt;proc = proc;
      thread-&gt;pid = current-&gt;pid;
      // 初始化thread的等待队列
      init_waitqueue_head(&amp;thread-&gt;wait);
      // 初始化thread的待处理事件列表
      INIT_LIST_HEAD(&amp;thread-&gt;todo);
      // 将该thread链接到proc-&gt;threads这棵红黑树中
      rb_link_node(&amp;thread-&gt;rb_node, parent, p);
      rb_insert_color(&amp;thread-&gt;rb_node, &amp;proc-&gt;threads);
      thread-&gt;looper |= BINDER_LOOPER_STATE_NEED_RETURN;
      thread-&gt;return_error = BR_OK;
      thread-&gt;return_error2 = BR_OK;
  }
  return thread;
}</code></pre><p>说明：<br>(01) 理解”红黑树”和”rb_entry”是理解while循环的前提。这里简单介绍下，proc-&gt;threads这棵红黑树是根据proc-&gt;thread-&gt;pid来排序的；而rb_entry(parent, struct binder_thread, rb_node)的作用根据binder_thread结构体对象中的已知成员的地址(binder_thread-&gt;rb_node的地址，也就是parent的值)来获取binder_thread结构体对象的地址。<br>(02) 很显然，由于之前没有创建过当前线程对应的binder_thread对象，所以*p==null为true。那么，接下来就新建binder_thread对象，并对其进行初始化，然后再添加到红黑树proc-&gt;threads中。</p>
<h3 id="6-3-Binder驱动中的binder-new-node-源码"><a href="#6-3-Binder驱动中的binder-new-node-源码" class="headerlink" title="6.3 Binder驱动中的binder_new_node()源码"></a>6.3 Binder驱动中的binder_new_node()源码</h3><p>下面看看binder_ioctl()中调用的binder_new_node()的代码。</p>
<pre><code>static struct binder_node *binder_new_node(struct binder_proc *proc,
                     void __user *ptr,
                     void __user *cookie)
{
  struct rb_node **p = &amp;proc-&gt;nodes.rb_node;
  struct rb_node *parent = NULL;
  struct binder_node *node;

  // 在proc-&gt;nodes这棵红黑树中，查找有要查找的binder实体(通过ptr成员来判断)
  while (*p) {
      parent = *p;
      node = rb_entry(parent, struct binder_node, rb_node);

      if (ptr &lt; node-&gt;ptr)
          p = &amp;(*p)-&gt;rb_left;
      else if (ptr &gt; node-&gt;ptr)
          p = &amp;(*p)-&gt;rb_right;
      else
          return NULL;
  }

  // 如果没有要找的binder实体，则新建该binder实体
  node = kzalloc(sizeof(*node), GFP_KERNEL);
  if (node == NULL)
      return NULL;
  binder_stats_created(BINDER_STAT_NODE);
  // 将node链接到红黑树proc-&gt;nodes中
  rb_link_node(&amp;node-&gt;rb_node, parent, p);
  rb_insert_color(&amp;node-&gt;rb_node, &amp;proc-&gt;nodes);
  node-&gt;debug_id = ++binder_last_id;
  // 将进程上下文信息保存到node-&gt;proc中
  node-&gt;proc = proc;
  node-&gt;ptr = ptr;
  node-&gt;cookie = cookie;
  node-&gt;work.type = BINDER_WORK_NODE;
  INIT_LIST_HEAD(&amp;node-&gt;work.entry);
  INIT_LIST_HEAD(&amp;node-&gt;async_todo);
  return node;
}</code></pre><p>说明：跟binder_get_thread()类似，这里是先在proc-&gt;nodes这棵红黑树中查找是否有binder实体(即binder_node对象)存在。有的话，返回NULL，即不需要新建binder实体；没有的话，则新建并初始化binder_node对象，然后将其添加到proc-&gt;nodes红黑树中。</p>
<p><br>至此，binder_become_context_manager()就介绍完了。它的作用：<br>(01) <strong>ServiceManager进程</strong>：告诉Kernel驱动，当前进程(即ServiceManager进程)是Binder上下文管理者。<br>(02) <strong>Binder驱动</strong>：新建当前线程对应的binder_thread对象，并将其添加到进程上下文信息binder_proc的threads红黑树中；新建ServiceManager对应的binder实体，并将该binder实体保存到全局变量binder_context_mgr_node中。</p>
<p><a name="anchor7"></a></p>
<h2 id="7-binder-loop"><a href="#7-binder-loop" class="headerlink" title="7. binder_loop()"></a>7. binder_loop()</h2><p>我们继续回到main()函数，分析一下binder_loop(bs, svcmgr_handler)。</p>
<h3 id="7-1-binder-loop-的源码"><a href="#7-1-binder-loop-的源码" class="headerlink" title="7.1 binder_loop()的源码"></a>7.1 binder_loop()的源码</h3><pre><code>void binder_loop(struct binder_state *bs, binder_handler func)
{
    int res; 
    struct binder_write_read bwr;
    unsigned readbuf[32];

    bwr.write_size = 0;
    bwr.write_consumed = 0;
    bwr.write_buffer = 0;

    // 告诉Kernel，ServiceManager进程进入了消息循环状态。
    readbuf[0] = BC_ENTER_LOOPER;
    binder_write(bs, readbuf, sizeof(unsigned));

    for (;;) {
        bwr.read_size = sizeof(readbuf);
        bwr.read_consumed = 0;
        bwr.read_buffer = (unsigned) readbuf;

        // 向Kernel中发送消息(先写后读)。
        // 先将消息传递给Kernel，然后再从Kernel读取消息反馈
        res = ioctl(bs-&gt;fd, BINDER_WRITE_READ, &amp;bwr);

        ...

        // 解析读取的消息反馈
        res = binder_parse(bs, 0, readbuf, bwr.read_consumed, func);
        ...
    }
}</code></pre><p>说明： 该代码定义在frameworks/native/cmds/servicemanager/binder.c中。<br>  binder_loop()首先调用binder_write(,BC_ENTER_LOOPER,)告诉Kernel，ServiceManager进入了消息循环状态。紧接着，就通过ioctl(,BINDER_WRITE_READ,)进入消息循环，等待Client发送请求(例如，MediaPlayer进程调用addService将MediaPlayer注册到ServiceManager中进行管理)。如果没有消息，则进入中断等待状态；如果有消息，则进行消息处理！ </p>
<h3 id="7-2-binder-write-的源码"><a href="#7-2-binder-write-的源码" class="headerlink" title="7.2 binder_write()的源码"></a>7.2 binder_write()的源码</h3><p>下面看看binder_loop()中的binder_write(,BC_ENTER_LOOPER,)。</p>
<pre><code>int binder_write(struct binder_state *bs, void *data, unsigned len)
{
    struct binder_write_read bwr;
    int res;
    bwr.write_size = len;                // 数据长度
    bwr.write_consumed = 0;             
    bwr.write_buffer = (unsigned) data;  // 数据是BINDER_WRITE_READ
    bwr.read_size = 0;
    bwr.read_consumed = 0;
    bwr.read_buffer = 0;
    res = ioctl(bs-&gt;fd, BINDER_WRITE_READ, &amp;bwr);
    if (res &lt; 0) {
        fprintf(stderr,&quot;binder_write: ioctl failed (%s)\n&quot;,
                strerror(errno));
    }
    return res;
}</code></pre><p>说明：binder_write()单单只是向Kernel发送一个消息，而不会去读取消息反馈。这里的ioctl()又会调用到binder_ioctl()。<br>这里涉及到了Binder通信中常用的数据结构体binder_write_read。bwr.write_size&gt;0，表示通过ServiceManager有数据(即BC_ENTER_LOOPER指令)发送给Binder驱动，而发送的数据就保存在bwr.write_buffer中，bwr.write_consumed则表示已经被读取并处理的数据的大小。bwr.read_XXX则是用来保存Binder驱动即将反馈给ServiceManager的信息的。<br>更多关于binder_write_read的介绍，请参考<a href="/2014/09/02/Binder-Datastruct/">Android Binder机制(二) Binder中的数据结构</a>。</p>
<h3 id="7-3-Binder驱动中binder-ioctl-的BINDER-WRITE-READ相关部分的源码"><a href="#7-3-Binder驱动中binder-ioctl-的BINDER-WRITE-READ相关部分的源码" class="headerlink" title="7.3 Binder驱动中binder_ioctl()的BINDER_WRITE_READ相关部分的源码"></a>7.3 Binder驱动中binder_ioctl()的BINDER_WRITE_READ相关部分的源码</h3><p>下面我们看看Binder驱动部分的对应代码。</p>
<pre><code>static long binder_ioctl(struct file *filp, unsigned int cmd, unsigned long arg)
{
  int ret;
  struct binder_proc *proc = filp-&gt;private_data;
  struct binder_thread *thread;
  unsigned int size = _IOC_SIZE(cmd);
  void __user *ubuf = (void __user *)arg;

  // 中断等待函数。
  // 1. 当binder_stop_on_user_error &lt; 2为true时；不会进入等待状态；直接跳过。
  // 2. 当binder_stop_on_user_error &lt; 2为false时，进入等待状态。
  //    当有其他进程通过wake_up_interruptible来唤醒binder_user_error_wait队列，并且binder_stop_on_user_error &lt; 2为true时；
  //    则继续执行；否则，再进入等待状态。
  ret = wait_event_interruptible(binder_user_error_wait, binder_stop_on_user_error &lt; 2);

  binder_lock(__func__);
  // 在proc进程中查找该线程对应的binder_thread；若查找失败，则新建一个binder_thread，并添加到proc-&gt;threads中。
  thread = binder_get_thread(proc);
  ...

  switch (cmd) {
  case BINDER_WRITE_READ: {
      struct binder_write_read bwr;
      ...

      // 将binder_write_read从&quot;用户空间&quot; 拷贝到 &quot;内核空间&quot;
      if (copy_from_user(&amp;bwr, ubuf, sizeof(bwr))) {
          ...
      }

      // 如果write_size&gt;0，则进行写操作
      if (bwr.write_size &gt; 0) {
          ret = binder_thread_write(proc, thread, (void __user *)bwr.write_buffer, bwr.write_size, &amp;bwr.write_consumed);
          ...
      }

      // 如果read_size&gt;0，则进行读操作
      if (bwr.read_size &gt; 0) {
          ret = binder_thread_read(proc, thread, (void __user *)bwr.read_buffer, bwr.read_size, &amp;bwr.read_consumed, filp-&gt;f_flags   &amp; O_NONBLOCK);
          ...
      }
      ...

      if (copy_to_user(ubuf, &amp;bwr, sizeof(bwr))) {
          ret = -EFAULT;
          goto err;
      }
      break;
  }
  ...
  }
  ret = 0;

  ...
  return ret;
}</code></pre><p>说明：<br>(01) wait_event_interruptible(binder_user_error_wait, binder_stop_on_user_error &lt; 2)中binder_stop_on_user_error &lt; 2为true。因此，不进入中断等待状态而是直接跳过该函数。<br>(02) thread = binder_get_thread(proc)。由于在上一次调用ioctl时，已经创建了该线程对应的binder_thread对象。因此，这次能在proc-&gt;threads红黑树中找到对应的binder_thread对象，然后，返回给thread。<br>(03) copy_from_user()的作用是将用户空间的数据拷贝到内核空间。即，将ServiceManager中调用ioctl(bs-&gt;fd, BINDER_WRITE_READ, &amp;bwr)时的bwr对象拷贝到Binder驱动中。<br>(04) 在binder_write()中，设置的bwr.write_size&gt;0；所以，调用binder_thread_write()进行写操作。<br>(05) 在binder_write()中，设置的bwr.read_size为0；所以，不调用binder_thread_read()进行读操作。<br>(06) 读写操作完毕之后，将bwr从内核空间再拷贝到用户空间。  </p>
<h3 id="7-4-Binder驱动中binder-thread-write-的源码"><a href="#7-4-Binder驱动中binder-thread-write-的源码" class="headerlink" title="7.4 Binder驱动中binder_thread_write()的源码"></a>7.4 Binder驱动中binder_thread_write()的源码</h3><pre><code>int binder_thread_write(struct binder_proc *proc, struct binder_thread *thread,
          void __user *buffer, int size, signed long *consumed)
{
  uint32_t cmd;
  void __user *ptr = buffer + *consumed;
  void __user *end = buffer + size;

  // 读取binder_write_read.write_buffer中的内容。
  // 每次读取32bit(即4个字节)
  while (ptr &lt; end &amp;&amp; thread-&gt;return_error == BR_OK) {
      // 从用户空间读取32bit到内核中，并赋值给cmd。
      if (get_user(cmd, (uint32_t __user *)ptr))
          return -EFAULT;

      ptr += sizeof(uint32_t);
      ...
      switch (cmd) {
      ...
      case BC_ENTER_LOOPER:
          ...
          // 设置线程的状态为BINDER_LOOPER_STATE_ENTERED；
          // 即，进入了循环状态
          thread-&gt;looper |= BINDER_LOOPER_STATE_ENTERED;
          break;
      ...
      }
      // 更新bwr.write_consumed的值
      *consumed = ptr - buffer;
  }
  return 0;
}</code></pre><p>说明：binder_thread_write()从brw.write_buffer中读取4个字节作为cmd。这4个字节就是ServiceManager传递的指令BC_ENTER_LOOPER。<br>在BC_ENTER_LOOPER对应的switch分支中，就是将BINDER_LOOPER_STATE_ENTERED加入到thread-&gt;looper中。即，告诉Binder驱动，ServiceManager进程进入了消息循环状态。</p>
<p><a name="anchor8"></a></p>
<h2 id="8-for"><a href="#8-for" class="headerlink" title="8. for(;;)"></a>8. for(;;)</h2><p>继续往下走。回到binder_loop()中后，便进入了for(;;)消息循环中。进入循环后，首先调用ioctl(,BINDER_WRITE_READ,)；此时，对应的bwr内容如下：</p>
<pre><code>bwr.write_size = 0;
bwr.write_consumed = 0;
bwr.write_buffer = 0;
bwr.read_size = sizeof(readbuf);
bwr.read_consumed = 0;
bwr.read_buffer = (unsigned) readbuf;</code></pre><p>bwr.write_size=0，而bwr.read_size&gt;0；表示只会从Binder驱动读取数据，而并不会向Binder驱动中写入数据。接着，调用ioctl()便再次进入到Binder驱动binder_ioctl()中。</p>
<h3 id="8-1-Binder驱动中binder-ioctl-的BINDER-WRITE-READ相关部分的源码"><a href="#8-1-Binder驱动中binder-ioctl-的BINDER-WRITE-READ相关部分的源码" class="headerlink" title="8.1 Binder驱动中binder_ioctl()的BINDER_WRITE_READ相关部分的源码"></a>8.1 Binder驱动中binder_ioctl()的BINDER_WRITE_READ相关部分的源码</h3><pre><code>static long binder_ioctl(struct file *filp, unsigned int cmd, unsigned long arg)
{
  ...
  switch (cmd) {
  case BINDER_WRITE_READ: {
      struct binder_write_read bwr;
      ...

      // 将binder_write_read从&quot;用户空间&quot; 拷贝到 &quot;内核空间&quot;
      if (copy_from_user(&amp;bwr, ubuf, sizeof(bwr))) {
          ...
      }

      // 如果write_size&gt;0，则进行写操作
      if (bwr.write_size &gt; 0) {
          ret = binder_thread_write(proc, thread, (void __user *)bwr.write_buffer, bwr.write_size, &amp;bwr.write_consumed);
          ...
      }

      // 如果read_size&gt;0，则进行读操作
      if (bwr.read_size &gt; 0) {
          ret = binder_thread_read(proc, thread, (void __user *)bwr.read_buffer, bwr.read_size, &amp;bwr.read_consumed, filp-&gt;f_flags   &amp; O_NONBLOCK);
          ...
      }
      ...

      if (copy_to_user(ubuf, &amp;bwr, sizeof(bwr))) {
          ret = -EFAULT;
          goto err;
      }
      break;
  }
  ...
  }
  ret = 0;

  ...
  return ret;
}</code></pre><p>说明：由于此次bwr.write_size=0，而bwr.read_size不为0。因此，在通过copy_from_user()将数据从用户空间拷贝到内核空间之后，不进行写操作，而只进行读操作，即只执行binder_thread_read()。 在读操作执行完毕之后，再通过copy_to_user()，将数据返回给用户空间。  </p>
<h3 id="8-2-Binder驱动中binder-thread-read-的源码"><a href="#8-2-Binder驱动中binder-thread-read-的源码" class="headerlink" title="8.2 Binder驱动中binder_thread_read()的源码"></a>8.2 Binder驱动中binder_thread_read()的源码</h3><pre><code>static int binder_thread_read(struct binder_proc *proc,
                struct binder_thread *thread,
                void  __user *buffer, int size,
                signed long *consumed, int non_block)
{
  void __user *ptr = buffer + *consumed;
  void __user *end = buffer + size;

  int ret = 0;
  int wait_for_proc_work;

  // 如果*consumed=0，则写入BR_NOOP到用户传进来的bwr.read_buffer缓存区
  if (*consumed == 0) {
      if (put_user(BR_NOOP, (uint32_t __user *)ptr))
          return -EFAULT;
      // 修改指针位置
      ptr += sizeof(uint32_t);
  }

retry:
  // 等待proc进程的事务标记。
  // 当线程的事务栈为空 并且 待处理事务列表为空时，该标记位true。
  wait_for_proc_work = thread-&gt;transaction_stack == NULL &amp;&amp;
              list_empty(&amp;thread-&gt;todo);

  ...

  // 设置线程为&quot;等待状态&quot;
  thread-&gt;looper |= BINDER_LOOPER_STATE_WAITING;
  if (wait_for_proc_work)
      proc-&gt;ready_threads++;

  ...
  if (wait_for_proc_work) {
      ...
      // 设置当前线程的优先级=proc-&gt;default_priority。
      // 即，当前线程要处理proc的事务，所以设置优先级和proc一样。
      binder_set_nice(proc-&gt;default_priority);
      if (non_block) {
          // 非阻塞式的读取，则通过binder_has_proc_work()读取proc的事务；
          // 若没有，则直接返回
          if (!binder_has_proc_work(proc, thread))
              ret = -EAGAIN;
      } else
          // 阻塞式的读取，则阻塞等待事务的发生。
          ret = wait_event_interruptible_exclusive(proc-&gt;wait, binder_has_proc_work(proc, thread));
  } else {
      ...
  }
  ...
}</code></pre><p>说明：<br>(01) 很显然，bwr.read_consumed=0。因此，*consumed=0，那么就将BR_NOOP拷贝到用户空间的bwr.read_buffer缓存区中。<br>(02) 目前为止，并没有进程将事务添加到当前线程中；因此，线程的事务栈和待处理事务队列都是为空。于是得到wait_for_proc_work的值是true。<br>(03) binder_set_nice()的作用是设置当前线程的优先级=proc-&gt;default_priority。<br>(04) 根据上下文，可知non_block为false。因此调用wait_event_interruptible_exclusive(proc-&gt;wait, binder_has_proc_work(proc, thread))。 而目前ServiceManager进程中没有待处理事务，因此binder_has_proc_work(proc, thread)为false。从而当前线程进入中断等待状态，等待其它进程将ServiceManager唤醒。 </p>
<p><br>至此，ServiceManager进入了等待状态，binder_loop()就分析就暂告一段落。<br>(01) <strong>ServiceManager进程</strong>：binder_loop()通过BC_ENTER_LOOPER告诉Kernel，ServiceManager进入了消息循环状态。接着，ServiceManager就进入等待状态，等待Client请求。<br>(02) <strong>Binder驱动</strong>：已知ServiceManager进入了消息循环状态；在收到ServiceManager的BINDER_WRITE_READ消息之后，就去ServiceManager的从进程上下文binder_proc对象中读取是否有待处理事务，由于没有事务处理，则将ServiceManager线程设为中断等待状态。  </p>
<p><a name="anchor_3rd"></a></p>
<h1 id="ServiceManager流程总结"><a href="#ServiceManager流程总结" class="headerlink" title="ServiceManager流程总结"></a>ServiceManager流程总结</h1><p>总结上面的分析，ServiceManager的main()进程完成了以下工作。 </p>
<ol>
<li><p>对于<strong>ServiceManager进程</strong>而言<br>它打开了Binder设备文件，并且将内存映射到ServiceManager的进程空间。然后，它告诉Binder驱动自己是Binder上下文的管理者。最后，进入消息循环，等待Client请求。</p>
</li>
<li><p>对于<strong>Binder驱动</strong>而言<br>初始化了ServiceManager对应的进程上下文环境(即binder_proc变量)，并将内核虚拟地址和进程虚拟地址映射到同一物理内存中。然后，新建当前线程对应的binder_thread对象，并将其添加到进程上下文信息binder_proc-&gt;threads红黑树中。在得知ServiceManager是Binder上下文管理者后，建立ServiceManager对应的Binder实体，并将该Binder实体保存到全局变量中。最后，得知ServiceManager进入消息循环后，由于当前线程中没有事务可处理，则进入中断等待状态，等待其他进程将其唤醒。</p>
</li>
</ol>

    
    </div>
    
    
</article>




    
    
        
<nav class="pagination is-centered is-rounded" role="navigation" aria-label="pagination">
    <div class="pagination-previous">
        <a href="/page/23/">Prev</a>
    </div>
    <div class="pagination-next">
        <a href="/page/25/">Next</a>
    </div>
    <ul class="pagination-list is-hidden-mobile">
        
        <li><a class="pagination-link" href="/">1</a></li>
        
        <li><span class="pagination-ellipsis">&hellip;</span></li>
        
        <li><a class="pagination-link" href="/page/23/">23</a></li>
        
        <li><a class="pagination-link is-current" href="/page/24/">24</a></li>
        
        <li><a class="pagination-link" href="/page/25/">25</a></li>
        
        <li><span class="pagination-ellipsis">&hellip;</span></li>
        
        <li><a class="pagination-link" href="/page/95/">95</a></li>
        
    </ul>
</nav>
    
    </div>
</section>
    <footer class="footer">
    <div class="container">
        <div class="columns content">
            <div class="column is-narrow has-text-centered">
                &copy; 2019 炸鸡叔&nbsp;
                Powered by <a href="http://hexo.io/" target="_blank">Hexo</a> & <a href="http://github.com/ppoffice/hexo-theme-minos">Minos</a>
            </div>
            <div class="column is-hidden-mobile"></div>

            
            <div class="column is-narrow">
                <div class="columns is-mobile is-multiline is-centered">
                
                    
                <a class="column is-narrow has-text-black" title="GitHub" href="https://github.com/ppoffice/hexo-theme-minos">
                    
                    GitHub
                    
                </a>
                
                </div>
            </div>
            
            
        </div>
    </div>
</footer>
    <script src="//cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
<script src="//cdnjs.cloudflare.com/ajax/libs/moment.js/2.22.2/moment-with-locales.min.js"></script>

<!-- test if the browser is outdated -->
<div id="outdated">
    <h6>Your browser is out-of-date!</h6>
    <p>Update your browser to view this website correctly. <a id="btnUpdateBrowser" href="http://outdatedbrowser.com/">Update my browser now </a></p>
    <p class="last"><a href="#" id="btnCloseUpdateBrowser" title="Close">&times;</a></p>
</div>
<script src="//cdnjs.cloudflare.com/ajax/libs/outdated-browser/1.1.5/outdatedbrowser.min.js"></script>
<script>
    $(document).ready(function () {
        // plugin function, place inside DOM ready function
        outdatedBrowser({
            bgColor: '#f25648',
            color: '#ffffff',
            lowerThan: 'flex'
        })
    });
</script>

<script>
    window.FontAwesomeConfig = {
        searchPseudoElements: true
    }
    moment.locale("zh-CN");
</script>


    
    
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML"></script>
<script>
    MathJax.Hub.Config({
        "HTML-CSS": {
            matchFontHeight: false
        },
        SVG: {
            matchFontHeight: false
        },
        CommonHTML: {
            matchFontHeight: false
        },
        tex2jax: {
            inlineMath: [
                ['$','$'],
                ['\\(','\\)']
            ]
        }
    });
</script>

    
    
    
    
<script src="//cdnjs.cloudflare.com/ajax/libs/lightgallery/1.6.8/js/lightgallery-all.min.js"></script>
<script src="//cdnjs.cloudflare.com/ajax/libs/justifiedGallery/3.6.5/js/jquery.justifiedGallery.min.js"></script>
<script>
    (function ($) {
        $(document).ready(function () {
            if (typeof($.fn.lightGallery) === 'function') {
                $('.article.gallery').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof($.fn.justifiedGallery) === 'function') {
                $('.justified-gallery').justifiedGallery();
            }
        });
    })(jQuery);
</script>

    
    
    <script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.0/clipboard.min.js"></script>
    <style>
        .hljs {
            position: relative;
        }

        .hljs .clipboard-btn {
            float: right;
            color: #9a9a9a;
            background: none;
            border: none;
            cursor: pointer;
        }

        .hljs .clipboard-btn:hover {
          color: #8a8a8a;
        }

        .hljs > .clipboard-btn {
            display: none;
            position: absolute;
            right: 4px;
            top: 4px;
        }

        .hljs:hover > .clipboard-btn {
            display: inline;
        }

        .hljs > figcaption > .clipboard-btn {
            margin-right: 4px;
        }
    </style>
    <script>
      $(document).ready(function () {
        $('figure.hljs').each(function(i, figure) {
          var codeId = 'code-' + i;
          var code = figure.querySelector('.code');
          var copyButton = $('<button>Copy <i class="far fa-clipboard"></i></button>');
          code.id = codeId;
          copyButton.addClass('clipboard-btn');
          copyButton.attr('data-clipboard-target-id', codeId);

          var figcaption = figure.querySelector('figcaption');

          if (figcaption) {
            figcaption.append(copyButton[0]);
          } else {
            figure.prepend(copyButton[0]);
          }
        })

        var clipboard = new ClipboardJS('.clipboard-btn', {
          target: function(trigger) {
            return document.getElementById(trigger.getAttribute('data-clipboard-target-id'));
          }
        });
        clipboard.on('success', function(e) {
          e.clearSelection();
        })
      })
    </script>

    
    

    


<script src="/js/script.js"></script>

    
    <div class="searchbox ins-search">
    <div class="searchbox-mask"></div>
    <div class="searchbox-container ins-search-container">
        <div class="searchbox-input-wrapper">
            <input type="text" class="searchbox-input ins-search-input" placeholder="Type something...">
            <span class="searchbox-close ins-close ins-selectable"><i class="fa fa-times-circle"></i></span>
        </div>
        <div class="searchbox-result-wrapper ins-section-wrapper">
            <div class="ins-section-container"></div>
        </div>
    </div>
</div>
<script>
    (function (window) {
        var INSIGHT_CONFIG = {
            TRANSLATION: {
                POSTS: 'Posts',
                PAGES: 'Pages',
                CATEGORIES: 'Categories',
                TAGS: 'Tags',
                UNTITLED: '(Untitled)',
            },
            CONTENT_URL: '/content.json',
        };
        window.INSIGHT_CONFIG = INSIGHT_CONFIG;
    })(window);
</script>
<script src="/js/insight.js"></script>
    
</body>
</html>